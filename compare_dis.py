# -*- coding: utf-8 -*-
"""
compare_dis.py â€” åˆ†å¸ƒå¼è®­ç»ƒç¨³å¥ç‰ˆ
- ä¸å†åŠ¨æ€æ”¹å†™ CUDA_VISIBLE_DEVICESï¼Œé¿å…è®¾å¤‡æ˜ å°„ç´Šä¹±
- åœ¨ import å‰è®¾ç½®å¸¸è§ NCCL çŽ¯å¢ƒå˜é‡ï¼Œé™ä½Žè¶…æ—¶/æ­»é”æ¦‚çŽ‡
- åˆ†å¸ƒå¼åˆå§‹åŒ–åŽæŒ‰ local_rank è®¾å®šå½“å‰ device
- monkeyâ€‘patch mmengine.logger çš„è®¾å¤‡èŽ·å–ä¸ºå½“å‰ CUDA è®¾å¤‡ï¼ˆä¸ä¾èµ– CVDï¼‰
  å•æœº4å¡ï¼š
    CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=29500 compare_dis.py
  å•å¡ï¼š
    CUDA_VISIBLE_DEVICES=0 python compare_dis.py
"""

# ========= â‘  åœ¨ import mmengine/mmaction ä¹‹å‰ï¼Œå…ˆè§„èŒƒåŒ–çŽ¯å¢ƒ =========
import os
import sys
import torch
import numpy as np

def _normalize_env_pre_import():
    # å¸¸è§ NCCL ç¨³å®šæ€§çŽ¯å¢ƒå˜é‡ï¼ˆå¦‚æœªæ˜¾å¼è®¾ç½®åˆ™ç»™é»˜è®¤å€¼ï¼‰
    os.environ.setdefault('TORCH_NCCL_BLOCKING_WAIT', '1')
    os.environ.setdefault('NCCL_ASYNC_ERROR_HANDLING', '1')
    # å•æœºå¤šå¡å¸¸è§é—®é¢˜ï¼šç¦ç”¨ IB/P2P å¯è§„é¿éƒ¨åˆ†çŽ¯å¢ƒä¸å…¼å®¹å¯¼è‡´çš„è¶…æ—¶ï¼ˆå¦‚éœ€è¦å¯æ‰‹åŠ¨è¦†ç›–ä¸º 0ï¼‰
    os.environ.setdefault('NCCL_IB_DISABLE', '1')
    os.environ.setdefault('NCCL_P2P_DISABLE', '1')

    # LOCAL_RANK æ¸…æ´—ï¼ˆä¸åŸºäºŽå®ƒåŽ»æ‰©å±•/æ”¹å†™ CVDï¼‰
    lr_raw = os.environ.get('LOCAL_RANK', '0')
    try:
        lr = int(lr_raw)
    except Exception:
        lr = 0
        os.environ['LOCAL_RANK'] = '0'

    # æ¸…æ´— CVDï¼šä»…åŽ»ç©ºæ ¼/ç©ºé¡¹ï¼Œä¸åšâ€œå¤åˆ¶æ‰©å±•â€ç­‰ç ´åæ€§æ”¹å†™
    cvd_raw = os.environ.get('CUDA_VISIBLE_DEVICES', '')
    parts = [p.strip() for p in cvd_raw.split(',') if p.strip() != '']
    if parts:
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(parts)

    # è¯Šæ–­æ‰“å°ï¼ˆä¸è§¦ç¢°å®žé™… CUDA è®¾å¤‡æžšä¸¾ï¼‰
    try:
        cuda_cnt = torch.cuda.device_count()
    except Exception:
        cuda_cnt = 0
    print(
        f"[pre-import] pid={os.getpid()} LR={os.environ.get('LOCAL_RANK')} "
        f"CVD={os.environ.get('CUDA_VISIBLE_DEVICES')} cuda_count={cuda_cnt}",
        file=sys.stderr,
    )

_normalize_env_pre_import()

# ========= â‘¡ åˆ†å¸ƒå¼åˆå§‹åŒ–ä¸Žå†æ¬¡å…œåº• =========
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ï¼ˆtorchrun æ³¨å…¥æ—¶å¯ç”¨ï¼‰ï¼Œè¿”å›ž local_rankã€‚"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        # env:// ä»Ž torchrun æ³¨å…¥çš„çŽ¯å¢ƒå˜é‡èŽ·å–åˆå§‹åŒ–ä¿¡æ¯
        # ä½¿ç”¨è¾ƒæ–°çš„ NCCL watchdog ç­–ç•¥ç”±ä¸Šé¢çš„çŽ¯å¢ƒå˜é‡æŽ§åˆ¶
        dist.init_process_group(backend='nccl', init_method='env://')
        local_rank = int(os.environ.get('LOCAL_RANK', '0'))
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
        print(
            f"[DDP] initialized rank={dist.get_rank()} / {dist.get_world_size()} (local_rank={local_rank})",
            file=sys.stderr,
        )
        return local_rank
    else:
        print("[Single GPU] non-distributed mode.", file=sys.stderr)
        return 0

def cleanup_distributed():
    if dist.is_initialized():
        dist.destroy_process_group()

# ========= â‘¢ çŽ°åœ¨ import mmengine.logger å¹¶ monkey-patch _get_device_id =========
import importlib
mm_logger_mod = importlib.import_module('mmengine.logging.logger')

def _safe_get_device_id():
    """è¿”å›žå½“å‰ CUDA è®¾å¤‡ç¼–å·ï¼›CPU æ—¶è¿”å›ž 0ã€‚ä¸ä¿®æ”¹ä»»ä½•çŽ¯å¢ƒå˜é‡ã€‚"""
    if torch.cuda.is_available():
        try:
            return int(torch.cuda.current_device())
        except Exception:
            return 0
    return 0

# ä»…æ›¿æ¢è®¾å¤‡èŽ·å–é€»è¾‘ï¼Œé¿å…å› è§£æž CVD è¶Šç•Œè€ŒæŠ¥é”™/æ”¹å†™çŽ¯å¢ƒ
mm_logger_mod._get_device_id = _safe_get_device_id  # type: ignore

# ========= â‘£ å…¶ä½™ä¾èµ–å†å¯¼å…¥ï¼ˆRunner ç­‰ï¼‰ =========
from mmengine.runner import Runner
from mmengine.config import Config
from mmengine.registry import init_default_scope
from mmaction.utils import register_all_modules

import torch.optim as optim

# ä½ çš„å·¥ç¨‹ä¾èµ–
from taa.metrics import AnticipationMetric
from ablation.CAP.src.model import accident  # CAP æ¨¡åž‹
from ablation.CAP.src.bert import opt



# ========= â‘¤ è®­ç»ƒ/è¯„ä¼°é€»è¾‘ =========
def one_hot_from_bool(target_bool_tensor, num_classes=2):
    t = target_bool_tensor.long().clamp(min=0, max=1)
    y = torch.zeros(t.size(0), num_classes, device=t.device, dtype=torch.float32)
    y.scatter_(1, t.view(-1, 1), 1.0)
    return y

def extract_batch_fields(data_batch, device):
    x = data_batch['inputs']                  # [B, C, T, H, W]
    data_samples = data_batch['data_samples'] # list[ActionDataSample]
    x = torch.cat(x, dim=0)
    x_cap = x.permute(2, 0, 1, 3, 4).contiguous().to(device)  # [B, T, C, H, W] =[1,60,3,224,224]
    x_cap = x_cap.float() / 255.0
    B = x_cap.size(0)
    targets, toa_frames = [], []

    ds=data_samples[0]
    def get_k(k, default=None):
        if hasattr(ds, k) and getattr(ds, k) is not None:
            return getattr(ds, k)
        if hasattr(ds, 'metainfo') and k in ds.metainfo:
            return ds.metainfo[k]
        if hasattr(ds, 'algorithms') and k in ds.algorithms:
            return ds.algorithms[k]
        return default


    target = int(bool(get_k('target', False)))
    fps = get_k('fps', 30) or 30
    accident_frame = get_k('accident_frame', None)
    start_index = get_k('start_index', 0)
    T = x_cap.size(1)
    toa = T * fps if accident_frame is None else max(int(accident_frame) - int(start_index), 0)
    targets.append(target)
    toa_frames.append(float(toa))

    y_onehot = one_hot_from_bool(torch.tensor(targets, device=device))
    toa = torch.tensor(toa_frames, device=device, dtype=torch.float32)
    texts = [""] * B
    return x_cap, y_onehot, toa, texts

@torch.no_grad()
def _eval_step_to_metric(preds_np, data_batch, metric, T):
    B = preds_np.shape[0]
    data_samples = []
    for i in range(B):
        ds = data_batch['data_samples'][i]
        def get_k(k, default=None):
            if hasattr(ds, k) and getattr(ds, k) is not None:
                return getattr(ds, k)
            if hasattr(ds, 'metainfo') and k in ds.metainfo:
                return ds.metainfo[k]
            if hasattr(ds, 'algorithms') and k in ds.algorithms:
                return ds.algorithms[k]
            return default
        result = dict(
            pred_score=torch.from_numpy(preds_np[i]),
            target=bool(get_k('target', False)),
            abnormal_start_frame=get_k('abnormal_start_frame', 0),
            accident_frame=get_k('accident_frame', 0),
            frame_inds=get_k('frame_inds', torch.arange(T).numpy()),
            video_id=get_k('video_id', f"video_{i}"),
            dataset=get_k('dataset', 'unknown'),
            frame_dir=get_k('frame_dir', ''),
            filename_tmpl=get_k('filename_tmpl', '{:06}.jpg'),
            type=get_k('type', ''),
            is_val=bool(get_k('is_val', False)),
            is_test=bool(get_k('is_test', False)),
        )
        data_samples.append(result)
    metric.process(None, data_samples)

def run_epoch(cap_model, loader, device, optimizer=None, metric=None, rank=0):
    train_mode = optimizer is not None
    cap_model.train(train_mode)
    total_loss, n = 0.0, 0
    for data_batch in loader:
        x_cap, y, toa, texts = extract_batch_fields(data_batch, device)
        losses, outputs = cap_model(x_cap, y, toa, texts)
        loss = losses['total_loss']

        if train_mode:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        total_loss += float(loss.detach().cpu())
        n += 1

        if (metric is not None) and (rank == 0):
            preds = torch.stack(outputs, dim=1).softmax(dim=-1)[..., 1].detach().cpu().numpy()
            _, T = preds.shape
            _eval_step_to_metric(preds, data_batch, metric, T)

    avg_loss = torch.tensor([total_loss / max(1, n)], device=device)
    if dist.is_initialized():
        dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
        avg_loss /= dist.get_world_size()
    return float(avg_loss.item())

# ========= â‘¥ ä¸»å‡½æ•° =========
def main():
    local_rank = setup_distributed()
    if torch.cuda.is_available():
        torch.cuda.set_device(local_rank)
        device = torch.device(f'cuda:{local_rank}')
    else:
        device = torch.device('cpu')

    from mmaction.utils import register_all_modules
    register_all_modules()

    cfg = Config.fromfile("configs/predict_anomaly_snippet.py")
    init_default_scope('mmaction')
    if not hasattr(cfg, 'launcher'):
        cfg.launcher = 'pytorch'
    if 'work_dir' not in cfg:
        cfg.work_dir = './work_dirs/cap_baseline'

    # è¿›ä¸€æ­¥æé«˜ç¨³å®šæ€§ï¼šé™åˆ¶ DataLoader å¹¶å‘å’Œå…³é—­æŒä¹…åŒ– workerï¼ˆå¸¸è§æ­»é”æºï¼‰
    if hasattr(cfg, 'train_dataloader') and isinstance(cfg.train_dataloader, dict):
        cfg.train_dataloader['num_workers'] = min(int(cfg.train_dataloader.get('num_workers', 4)), 4)
        cfg.train_dataloader['persistent_workers'] = False
        # ä¿éšœå„ rank æ­¥æ•°ä¸€è‡´
        if isinstance(cfg.train_dataloader.get('sampler', {}), dict):
            cfg.train_dataloader['sampler']['round_up'] = True
    if hasattr(cfg, 'val_dataloader') and isinstance(cfg.val_dataloader, dict):
        cfg.val_dataloader['num_workers'] = min(int(cfg.val_dataloader.get('num_workers', 4)), 4)
        cfg.val_dataloader['persistent_workers'] = False
        if isinstance(cfg.val_dataloader.get('sampler', {}), dict):
            cfg.val_dataloader['sampler']['round_up'] = True

    # å…³é”®ï¼šçŽ°åœ¨æ‰åˆ›å»º Runnerï¼ˆlogger å·²è¢« monkey-patchï¼‰
    runner = Runner.from_cfg(cfg)

    global_rank = dist.get_rank() if dist.is_initialized() else 0
    is_main = (global_rank == 0)
    if is_main:
        os.makedirs(cfg.work_dir, exist_ok=True)

    train_loader = runner.build_dataloader(cfg.train_dataloader)
    val_loader = runner.build_dataloader(cfg.val_dataloader)

    # ä¿è¯æ‰€æœ‰ rank åœ¨è¿›å…¥è®­ç»ƒå¾ªçŽ¯å‰åŒæ­¥ï¼Œé¿å…æŸäº› rank å…ˆè¿›å…¥åå‘å¯¼è‡´çš„ç­‰å¾…
    if dist.is_initialized():
        dist.barrier()

    cap_model = accident(
        h_dim=opt.s_dim2,
        n_layers=1,
        depth=opt.tran_num_layers,
        adim=opt.adim,
        heads=opt.heads,
        num_tokens=opt.num_tokens,
        c_dim=opt.c_dim,
        s_dim1=opt.s_dim1,
        s_dim2=opt.s_dim2,
        keral=opt.keral,
        num_class=opt.num_class
    ).to(device)

    if dist.is_initialized():
        cap_model = DDP(
            cap_model,
            device_ids=[local_rank],
            find_unused_parameters=True,
            broadcast_buffers=False,
            gradient_as_bucket_view=True,
        )

    optimizer = optim.AdamW(cap_model.parameters(), lr=1e-4, weight_decay=1e-4)
    metric = AnticipationMetric(fpr_max=0.1, output_dir="outputs") if is_main else None

    max_epochs = 30
    best_mAUC = -1.0
    best_epoch = -1
    save_dir = "outputs"
    if is_main:
        os.makedirs(save_dir, exist_ok=True)

    for epoch in range(1, max_epochs + 1):
        if dist.is_initialized():
            sampler = getattr(train_loader, 'sampler', None)
            if hasattr(sampler, 'set_epoch'):
                sampler.set_epoch(epoch)
            val_sampler = getattr(val_loader, 'sampler', None)
            if hasattr(val_sampler, 'set_epoch'):
                val_sampler.set_epoch(epoch)

        train_loss = run_epoch(cap_model, train_loader, device, optimizer=optimizer, metric=None, rank=global_rank)
        if is_main:
            print(f"[Epoch {epoch}] Train Loss: {train_loss:.4f}")

        if dist.is_initialized():
            dist.barrier()

        if is_main and metric is not None:
            if hasattr(metric, "reset"):
                metric.reset()
            elif hasattr(metric, "results"):
                metric.results.clear()
            metric.epoch = epoch

        if dist.is_initialized():
            dist.barrier()

        val_metric = metric if is_main else None
        with torch.no_grad():
            _ = run_epoch(cap_model, val_loader, device, optimizer=None, metric=val_metric, rank=global_rank)

        if dist.is_initialized():
            dist.barrier()

        if is_main and metric is not None:
            raw_results = metric.compute_metrics(metric.results)
            if metric.prefix:
                results = {f"{metric.prefix}/{k}": v for k, v in raw_results.items()}
            else:
                results = raw_results
            metric.results.clear()
            print(f"[Epoch {epoch}] Val Results: {results}")

            mauc_key = "mAUC@" if "mAUC@" in results else ("mAUC#" if "mAUC#" in results else None)
            current_mAUC = results.get(mauc_key, 0.0) if mauc_key else 0.0
            if current_mAUC > best_mAUC:
                best_mAUC = current_mAUC
                best_epoch = epoch
                best_path = f"{save_dir}/best_mAUC_epoch_{epoch:03d}_{best_mAUC:.4f}.pth"
                torch.save({
                    "epoch": epoch,
                    "model_state_dict": cap_model.module.state_dict() if hasattr(cap_model, 'module') else cap_model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "best_mAUC": best_mAUC
                }, best_path)
                print(f"âœ… [Epoch {epoch}] New best mAUC = {best_mAUC:.4f}, saved to {best_path}")

    if is_main:
        print(f"\nðŸŽ¯ Training done. Best mAUC = {best_mAUC:.4f} at epoch {best_epoch}")

    cleanup_distributed()

if __name__ == '__main__':
    main()
